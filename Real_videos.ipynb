{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x166fe1daa90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from src.models import models\n",
    "from src.models import model as mainmodel\n",
    "from src.models import modelConv2d\n",
    "from src.models import modelineal\n",
    "from src.models import decoders\n",
    "from src import loss_func\n",
    "from src import train\n",
    "from src import loader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src import optimizer_Factory as of\n",
    "import Data.genData as genData\n",
    "from src import custom_plots as cp\n",
    "from src import Visual_utils as vu\n",
    "import torchvision\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2000\"\n",
    "torch.cuda.empty_cache() \n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate over folders and process video files\n",
    "def iterate_and_process_videos(base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # If no subdirectories, we are in a terminal folder\n",
    "        if not dirs:\n",
    "            # Iterate over mp4 files that don't contain 'mask' in the filename\n",
    "            mp4_files = [f for f in files if f.endswith('.mp4') and 'mask' not in f]\n",
    "            for mp4_file in mp4_files:\n",
    "                video_path = os.path.join(root, mp4_file)\n",
    "                numpy_save_path = os.path.join(root, f\"{os.path.basename(root)}.npy\")\n",
    "                video_array = process_video(video_path, new_width=100, new_height=56)\n",
    "                np.save(numpy_save_path, video_array)\n",
    "\n",
    "# Function to process video: resize, convert to grayscale, and normalize\n",
    "def process_video(video_path, new_width, new_height):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize the frame\n",
    "        resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "        # Convert the frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Normalize the frame to range [0, 1]\n",
    "        normalized_frame = gray_frame / 255.0\n",
    "        frames.append(normalized_frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Convert list of frames to numpy array\n",
    "    video_array = np.array(frames)\n",
    "    return video_array\n",
    "\n",
    "# Function to calculate mean and std of alpha and beta based on folder1 and folder2\n",
    "def calculate_mean_std(base_path):\n",
    "    csv_file_path = os.path.join(base_path, \"experiment_results.csv\")\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"Error: CSV file {csv_file_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Read CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Group by folder1 and folder2, and calculate mean and std for alpha and beta\n",
    "    grouped = df.groupby(['folder1', 'folder2']).agg(\n",
    "        alpha_mean=('alpha', 'mean'),\n",
    "        alpha_std=('alpha', 'std'),\n",
    "        beta_mean=('beta', 'mean'),\n",
    "        beta_std=('beta', 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Save the results to a new CSV file\n",
    "    output_csv_path = os.path.join(base_path, \"mean_std_results.csv\")\n",
    "    grouped.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Mean and std results saved to {output_csv_path}\")\n",
    "\n",
    "# Function to execute experiment for a particular file and update CSV\n",
    "def execute_experiment_for_file(base_path, folder1, folder2, folder):\n",
    "    csv_file_path = os.path.join(base_path, \"experiment_results.csv\")\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"Error: CSV file {csv_file_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Read CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    print(f\"Executing experiment for folder1={folder1}, folder2={folder2}, folder={folder}\")\n",
    "\n",
    "    #print(df)\n",
    "\n",
    "    # Get the path of the numpy file\n",
    "    npy_file_path = os.path.join(base_path, folder1, folder2, folder, f\"{folder}.npy\")\n",
    "\n",
    "    print(f\"Processing file {npy_file_path}\")\n",
    "    \n",
    "    # Find the row that matches folder1, folder2, and folder\n",
    "    matching_index = df[(df['folder1'] == folder1) & (df['folder2'] == folder2) & (df['folder'] == int(folder) )].index\n",
    "    if matching_index.empty:\n",
    "        print(f\"Error: No matching row found for folder1={folder1}, folder2={folder2}, folder={folder}\")\n",
    "        return\n",
    "    \n",
    "    # Get the path of the numpy file\n",
    "    npy_file_path = os.path.join(base_path, folder1, folder2, folder, f\"{folder}.npy\")\n",
    "    if not os.path.exists(npy_file_path):\n",
    "        print(f\"Error: Numpy file {npy_file_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Execute experiment function\n",
    "    alpha, beta = test_execute_experiment(npy_file_path, dynamics=folder1, dt=0.1)\n",
    "    \n",
    "    # Update the row in the DataFrame\n",
    "    df.at[matching_index[0], 'alpha'] = alpha\n",
    "    df.at[matching_index[0], 'beta'] = beta\n",
    "    \n",
    "    # Save the updated DataFrame back to CSV\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Updated row for folder1={folder1}, folder2={folder2}, folder={folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./Data/Real_videos\"  # Change to your base folder path\n",
    "iterate_and_process_videos(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiment(path, dynamics, dt):\n",
    "\n",
    "    torch.cuda.empty_cache() \n",
    "    torch.manual_seed(42)\n",
    "\n",
    "\n",
    "    data_folder = np.load(path, allow_pickle=True)\n",
    "    data_train = data_folder\n",
    "    # print max\n",
    "    print(np.max(data_train))\n",
    "    print(data_train.shape)\n",
    "\n",
    "    #dt = 3/30\n",
    "    train_dataloader, test_dataloader, train_x, val_x  = loader.getLoader_folder(data_train, split=True)\n",
    "\n",
    "\n",
    "\n",
    "    latentEncoder_I = mainmodel.EndPhys(dt = dt,\n",
    "                                    pmodel = dynamics,\n",
    "                                    init_phys = 1.0, \n",
    "                                    initw=True)\n",
    "\n",
    "    latentEncoder_I, log, params  = train.train(latentEncoder_I, \n",
    "                                    train_dataloader, \n",
    "                                    test_dataloader,\n",
    "                                    init_phys = 0.1,                                 \n",
    "                                    loss_name='latent_loss')\n",
    "def test_execute_experiment(path, dynamics, dt):\n",
    "    # This is a placeholder implementation. Replace with the actual logic.\n",
    "    alpha = np.random.rand()  # Random value as a placeholder\n",
    "    beta = np.random.rand()   # Random value as a placeholder\n",
    "    return alpha, beta\n",
    "  \n",
    "def iterate_and_execute_experiment(base_path):\n",
    "    csv_file_path = os.path.join(base_path, \"experiment_results.csv\")\n",
    "    with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow([\"folder1\", \"folder2\", \"folder\", \"alpha\", \"beta\"])\n",
    "        \n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            # Iterate over .npy files\n",
    "            npy_files = [f for f in files if f.endswith('.npy')]\n",
    "            for npy_file in npy_files:\n",
    "                npy_file_path = os.path.join(root, npy_file)\n",
    "                folder_hierarchy = root.split(os.sep)[-3:]\n",
    "                folder1 = folder_hierarchy[0] if len(folder_hierarchy) > 0 else \"\"\n",
    "                folder2 = folder_hierarchy[1] if len(folder_hierarchy) > 1 else \"\"\n",
    "                folder = folder_hierarchy[2] if len(folder_hierarchy) > 2 else \"\"\n",
    "                \n",
    "                # Execute experiment function\n",
    "                alpha, beta = test_execute_experiment(npy_file_path, dynamics=folder1, dt=0.1)\n",
    "                \n",
    "                # Write results to CSV\n",
    "                csv_writer.writerow([folder1, folder2, folder, alpha, beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and std results saved to ./Data/Real_videos\\mean_std_results.csv\n"
     ]
    }
   ],
   "source": [
    "base_path = \"./Data/Real_videos\"  # Change to your base folder path\n",
    "iterate_and_execute_experiment(base_path)\n",
    "calculate_mean_std(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing experiment for folder1=bouncing_ball, folder2=table, folder=01\n",
      "Processing file ./Data/Real_videos\\bouncing_ball\\table\\01\\01.npy\n",
      "Updated row for folder1=bouncing_ball, folder2=table, folder=01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder1 = \"bouncing_ball\"\n",
    "folder2 = \"table\"\n",
    "folder = \"01\"\n",
    "execute_experiment_for_file(base_path, folder1, folder2, folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
